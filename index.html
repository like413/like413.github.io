<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="robots" content="index, follow" />
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Ke Li, ææŸ¯, Remote Sensing, Image Processing, Deep Learning, Multi-Modality, Xidian University">
<link rel="stylesheet" href="./Files/jemdoc.css" type="text/css" />
<script src="jquery.min.js"></script>
<link rel="shortcut icon" href="./Files/favicon.ico">
<title>Ke Li's Homepage</title>
</head>

<body>

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 

<table class="imgtable"><tr><td>
<a href="./"><img src="./Files/like-life.png" alt="" height="215px" /></a>&nbsp;</td>
<td align="left"><p><a href="./"><font size="4">Ke Li(</font><font size="4"; font style="font-family:Microsoft YaHei">ææŸ¯-è¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦</font><font size="4">)</font></a><br />
<i>Ph.D. student (2023 ~ Present), advised by Prof. <a href="https://web.xidian.edu.cn/wangdi/" target="_blank">Di Wang</a></i>
<br /><br />
 
<a href="https://cs.xidian.edu.cn/ztwz/z2024yzzt/sy/qrsjsjsyjs/ky.htm" target="_blank">Key Laboratory of Smart Human-Computer Interaction and Wearable Technology of Shaanxi Province</a><br />
<a href="https://cs.xidian.edu.cn/" target="_blank">School of Computer Science and Technology, Xidian University</a><br />
<br />
Email: like0413@stu.xidian.edu.cn <br />
[<a href="https://github.com/like413" target="_blank">GitHub</a>] 
[<a href="https://scholar.google.com/citations?user=xidIhU0AAAAJ&hl=zh-CN" target="_blank">GoogleScholar</a>] 
[<a href="https://orcid.org/my-orcid?orcid=0000-0003-2873-7795" target="_blank">ORCID</a>] 
 
<br />
<br />
 Location: 266 Xinglong Section of Xifeng Road, Xi'an, Shaanxi, China 710126<br />
<class="staffshortcut">
 <A HREF="#News">News</A> | 
 <A HREF="#Interest">Research Interest</A> | 
 <!-- <A HREF="#Co-authors">Co-authors</A> | -->
 <!-- <A HREF="#Education">Education</A> |  -->
 <A HREF="#Publications">Publications</A> | 
 <A HREF="#Membership">Membership</A> | 
 <A HREF="#Academic Service">Academic Service</A> | 
 <A HREF="#Awards">Awards</A>|
 <A HREF="#Conference Experience">Conference Experience</A>

</td></tr></table>

<A NAME="News"><h2>ğŸ—ï¸ News</h2></A>
<div class="news" style="overflow:auto; height:200px; Width:99%;">
 <ul>
 <li>[12, 2025]&nbsp;&nbsp;&nbsp; è·è¯„ 2025å¹´è¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦<h7 style="color: #f44597;">åä¸ºå¥–å­¦é‡‘</h7> (å…¨æ ¡ä»…3äºº).</li>
 <li>[12, 2025]&nbsp;&nbsp;&nbsp; å…¥é€‰ <h7 style="color: #f44597;">2025å¹´åº¦ä¸­å›½ç§‘åé’å¹´ç§‘æŠ€äººæ‰åŸ¹è‚²å·¥ç¨‹åšå£«ç”Ÿä¸“é¡¹è®¡åˆ’</h7>.</li>
 <li>[11, 2025]&nbsp;&nbsp;&nbsp; 1 paper accepted to <h7 style="color: #f44597;">AAAI 2026</h7>.</li>
 <li>[10, 2025]&nbsp;&nbsp;&nbsp; 1 paper accepted to <h7>CCF ICCC 2025, and awarded <h7 style="color: #f44597;">the A-Class Paper Award</h7></h7>.</li>
 <li>[09, 2025]&nbsp;&nbsp;&nbsp; 1 paper accepted to <h7>IEEE JSTARS</h7>.</li>
 <li>[01, 2025]&nbsp;&nbsp;&nbsp; Several papers accepted, 1 <h7>IEEE IoT</h7>, 1 <h7>PRL</h7>.</li>
 <li>[12, 2024]&nbsp;&nbsp;&nbsp; 1 paper accepted to <h7 style="color: #f44597;">AAAI 2025</h7>.</li>
 <li>[11, 2024]&nbsp;&nbsp;&nbsp; 1 paper accepted to <h7 style="color: #f44597;">IEEE TIFS</h7>.</li>
 <li>[08, 2024]&nbsp;&nbsp;&nbsp; 1 paper accepted to <h7>NN</h7>.</li>
 <li>[07, 2024]&nbsp;&nbsp;&nbsp; Several papers accepted, 1 <h7>IEEE TGRS</h7>, 1 <h7>PR</h7>, 3 <h7>IGARSS 2024</h7>.</li>
 <li>[06, 2024]&nbsp;&nbsp;&nbsp; 1 paper accepted to <h7 style="color: #f44597;">CVPR 2024</h7>.</li>
 <li>[04, 2024]&nbsp;&nbsp;&nbsp; 1 paper accepted to <h7>IEEE GRSL</h7>.</li>
 <li>[03, 2024]&nbsp;&nbsp;&nbsp; 1 paper accepted to <h7>IEEE JSTARS</h7>.</li>
 <li>[02, 2024]&nbsp;&nbsp;&nbsp; 1 paper accepted to <h7>ESWA</h7>.</li>                
 <li>[08, 2023]&nbsp;&nbsp;&nbsp; 1 paper accepted to <h7>IEEE TGRS</h7>.</li>
 <li>[06, 2023]&nbsp;&nbsp;&nbsp; 1 paper accepted to <h7>BSPC</h7>.</li>
 </ul>
</div>
<br />    

 
<!-- <A NAME="News"><h2>News</h2></A>
<ul>
 <li> <b> <font color="#FF0000">[2024.06]</font> </b>The biggest Hyperspectral foundation model <b>HyperSIGMA</b> has been released at Arxiv [<a href="https://arxiv.org/abs/2406.11519">Paper</a>][<a href="https://github.com/WHU-Sigma/HyperSIGMA">Code</a>]!
 <li> <b> <font color="#FF0000">[2024.06]</font> </b> I am gonna to a postdoc at Sun Yat-sen University !</li>
 <li> <b> <font color="#FF0000">[2024.05]</font> </b> One paper has been accepted by <b>ISPRS</b> (<a href="https://www.sciencedirect.com/science/article/pii/S0924271624001539" target="_blank">link</a>)!</li>
 <li> <b> <font color="#FF0000">[2024.02]</font> </b> One co-authored paper has been accepted by <b>IEEE TGRS</b> (<a href="https://ieeexplore.ieee.org/document/10445496" target="_blank">link</a>)!</li>
 <li> <b> <font color="#FF0000">[2023.10]</font> </b> One co-authored paper has been accepted by <b>IEEE JSTARS</b> (<a href="https://ieeexplore.ieee.org/abstract/document/10285305" target="_blank">link</a>)!</li>
 <li> <b> <font color="#FF0000">[2023.09]</font> </b> One co-authored paper has been accepted by <b>IEEE JSTARS</b> (<a href="https://ieeexplore.ieee.org/document/10234560" target="_blank">link</a>)!</li>
 <li> <b> <font color="#FF0000">[2023.04]</font> </b> One co-authored paper has been accepted by <b>IEEE JSTARS</b> (<a href="https://ieeexplore.ieee.org/abstract/document/10093022" target="_blank">link</a>)!</li>
 <li> <b> <font color="#FF0000">[2023.04]</font> </b> One paper is acceptted as a <b>Poster</b> by <b>IGARSS 2023</b> (<a href="https://2023.ieeeigarss.org/papers/accepted_papers.php" target="_blank">homepage)</a>!</li>

</ul>
<br /> -->


 
<A NAME="Interest"><h2>ğŸŒŸ Research Interest</h2></A>
<!-- I am working in Computer Vision; Multimodal Learning; Remote Sensing Intelligent Interpretation. Currently, I focus on the following research topics: -->
<ul>
<li><b>Computer Vision:</b> Image Classification, Detection, Segmentation, etc.</li>
<li><b>Remote Sensing Intelligent Interpretation:</b> Object Detection, Visual Grounding, Change Detection, etc.</li>
<li><b>Multimodal Learning:</b> Vision-Language Models, Visual Question Answering, etc.</li>
</ul>
<br />

<A NAME="Publications"><h2>ğŸ“‘ Publications</h2></A>
<!-- <p><b>All publications can be found at <a href="https://scholar.google.com/citations?user=xidIhU0AAAAJ&hl=zh-CN" target="_blank">Here</a>.<br/></b></p>    -->

<p><b>Datasets</b>: </p>
<font size="3"> 
<ul>
</ul>
</font>

<p></p>
<font size="3"> 
<ul>

<table width="100%" class="imgtable">
<tr>
    <td width="306"> <img src="Files/OPT-RSVG.jpg" width="290px"></td>
    <td>
        <a href="https://github.com/like413/OPT-RSVG">OPT-RSVG: A Challenging Large-Scale Remote Sensing Visual Grounding Dataset</a>
        <br><b>Ke Li</b>, Di Wang, Xu Wang, Gang Liu, Zili Wu, Quan Wang
        <br><i>IEEE Transactions on Geoscience and Remote Sensing (<b>TGRS</b>)</i>, 2023. (<b>SCI Q1 TOP, IF=8.2</b>) 
        <br>[<a href="https://like413.github.io/OPT-RSVG/">Project Page</a>][<a href="https://ieeexplore.ieee.org/abstract/document/10584552">Paper</a>][<a href="https://github.com/like413/OPT-RSVG">Dataset and Code</a>]
    </td>
</tr>
</table>
<br />

<table width="100%" class="imgtable">
    <tr>
        <td width="306"> <img src="Files/CDQAG.jpg" width="290px"></td>
        <td>
            <a href="https://like413.github.io/CDQAG/">QAG-360K: The First Change Detection Question Answering and Grounding Dataset</a>
            <br> <b>Ke Li</b>, Fuyu Dong, Di Wang, Shaofeng Li, Quan Wang, Xinbo Gao, Tat-Seng Chua
            <br><i>arXiv preprint arXiv</i>, 2024. 
            <br>[<a href="https://like413.github.io/CDQAG/">Project Page</a>][<a href="https://arxiv.org/abs/2410.23828">Paper</a>][<a href="https://github.com/like413/VisTA">Dataset and Code</a>]
        </td>
    </tr>
</table>
<br />

</ul>
</ul>
<p></p>

<p><b>Papers</b>: </p>
<font size="3"> 
<ul>
</ul>
</font>

<p></p>
<font size="3"> 
<ul>


<table width="100%" class="imgtable">
    <tr>
        <td width="306"> <img src="Files/RSVG-ZeorOV.jpg" width="290px"></td>
        <td>
            <a href="https://arxiv.org/abs/2509.18711">[9] RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images</a>
            <br> <b>Ke Li</b>, Di Wang, Ting Wang, Fuyu Dong, Yiming Zhang, Luyao Zhang, Xiangyu Wang, Shaofeng Li, Quan Wang
            <br><i>The 40th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</i>, 2026. (<b>CCF-A</b>)
            <br>[<a href="https://arxiv.org/abs/2509.18711">Paper</a>][<a href="https://github.com/like413/RSVG-ZeorOV">Code</a>]
        </td>
    </tr>
</table>
<br />

<table width="100%" class="imgtable">
    <tr>
        <td width="306"> <img src="Files/CVD.jpg" width="290px"></td>
        <td>
            <a href="https://arxiv.org/abs/2505.11822v2">[8] Robust Drone-View Geo-Localization via Content-Viewpoint Disentanglement</a>
            <br> <b>Ke Li</b>, Di Wang, Xiaowei Wang, Zhihong Wu, Yiming Zhang, Yifeng Wang, Quan Wang
            <br><i>arXiv preprint arXiv</i>, 2025. 
            <br>[<a href="https://arxiv.org/pdf/2505.11822v2">Paper</a>]
        </td>
    </tr>
</table>
<br />
    
<table width="100%" class="imgtable">
    <tr>
        <td width="306"> <img src="Files/FD2-Net.jpg" width="290px"></td>
        <td>
            <a href="">[7] FD2-Net: Frequency-Driven Feature Decomposition Network for Infrared-Visible Object Detection</a>
            <br> <b>Ke Li</b>, Di Wang, Zhangyuan Hu, Shaofeng Li, Weiping Ni, Lin Zhao, Quan Wang
            <br><i>The 39th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</i>, 2025. (<b>CCF-A</b>) 
            <br>[<a href="https://arxiv.org/abs/2412.09258">Paper</a>]
        </td>
    </tr>
</table>
<br />

<table width="100%" class="imgtable">
    <tr>
        <td width="306"> <img src="Files/VisTA.jpg" width="290px"></td>
        <td>
            <a href="">[6] Show Me What and Where has Changed? Question Answering and Grounding for Remote Sensing Change Detection</a>
            <br> <b>Ke Li</b>, Fuyu Dong, Di Wang, Shaofeng Li, Quan Wang, Xinbo Gao, Tat-Seng Chua
            <br><i>arXiv preprint arXiv</i>, 2024. 
            <br>[<a href="https://arxiv.org/abs/2410.23828">Paper</a>][<a href="https://github.com/like413/VisTA">Dataset and Code</a>]
        </td>
    </tr>
</table>
<br />

<table width="100%" class="imgtable">
    <tr>
        <td width="306"> <img src="Files/SFS-Conv.jpg" width="290px"></td>
        <td>
            <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Unleashing_Channel_Potential_Space-Frequency_Selection_Convolution_for_SAR_Object_Detection_CVPR_2024_paper.pdf">[5] Unleashing Channel Potential: Space-Frequency Selection Convolution for SAR Object Detection</a>
            <br> <b>Ke Li</b>, Di Wang, Zhangyuan Hu, Wenxuan Zhu, Shaofeng Li, Quan Wang
            <br><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</i>, 2024. (<b>CCF-A</b>) 
            <br>[<a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_Unleashing_Channel_Potential_Space-Frequency_Selection_Convolution_for_SAR_Object_Detection_CVPR_2024_paper.html">Paper</a>][<a href="https://github.com/like413/SFS-Conv">Code</a>][<a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31673.png?t=1717081744.6876934">Poster</a>]
        </td>
    </tr>
</table>
<br /> 

<table width="100%" class="imgtable">
<tr>
    <td width="306"> <img src="Files/PatchGen.jpg" width="290px"></td>
    <td>
        <a href="https://ieeexplore.ieee.org/document/10795220">[4] Physical Adversarial Patch Attack for Optical Fine-grained Aircraft Recognition</a>
        <br><b>Ke Li</b>, Di Wang, Wenxuan Zhu, Shaofeng Li, Quan Wang, Xinbo Gao
        <br><i> IEEE Transactions on Information Forensics and Security (<b>TIFS</b>)</i>, 2024. (<b>CCF-A, SCI Q1 TOP, IF=6.3</b>)
        <br>[<a href="https://ieeexplore.ieee.org/document/10795220">Paper</a>]
    </td>
</tr>
</table>
<br />

<table width="100%" class="imgtable">
<tr>
    <td width="306"> <img src="Files/LPVA.jpg" width="290px"></td>
    <td>
        <a href="https://ieeexplore.ieee.org/abstract/document/10584552">[3] Language-Guided Progressive Attention for Visual Grounding in Remote Sensing Images</a>
        <br><b>Ke Li</b>, Di Wang, Haojie Xu, Haodi Zhong, Cong Wang
        <br><i> IEEE Transactions on Geoscience and Remote Sensing (<b>TGRS</b>)</i>, 2024. (<b>SCI Q1, IF=7.5</b>)
        <br>[<a href="https://like413.github.io/OPT-RSVG/">Project Page</a>][<a href="https://ieeexplore.ieee.org/abstract/document/10584552">Paper</a>][<a href="https://github.com/like413/OPT-RSVG">Dataset and Code</a>]
    </td>
</tr>
</table>
<br />

<table width="100%" class="imgtable">
<tr>
    <td width="306"> <img src="Files/DiagSwin.jpg" width="290px"></td>
    <td>
        <a href="https://www.sciencedirect.com/science/article/pii/S089360802400577X?via%3Dihub">[2] DiagSWin: A Multi-Scale Vision Transformer with Diagonal-Shaped Windows for Object Detection and Segmentation</a>
        <br><b>Ke Li</b>, Di Wang, Gang Liu, Wenxuan Zhu, Haodi Zhong, Quan Wang
        <br><i> Neural Networks (<b>NN</b>)</i>, 2024. (<b>SCI Q1 TOP, IF=6.0</b>)
        <br>[<a href="https://www.sciencedirect.com/science/article/pii/S089360802400577X?via%3Dihub">Paper</a>]
    </td>
</tr>
</table>
<br />

<table width="100%" class="imgtable">
<tr>
    <td width="306"> <img src="Files/MACN.jpg" width="290px"></td>
    <td>
        <a href="https://ieeexplore.ieee.org/abstract/document/10236462">[1] Mixing Self-Attention and Convolution: A Unified Framework for Multisource Remote Sensing Data Classification</a>
        <br><b>Ke Li</b>, Di Wang, Xu Wang, Gang Liu, Zili Wu, Quan Wang
        <br><i>IEEE Transactions on Geoscience and Remote Sensing (<b>TGRS</b>)</i>, 2023. (<b>SCI Q1 TOP, IF=8.2</b>) 
        <br>[<a href="https://ieeexplore.ieee.org/abstract/document/10236462">Paper</a>][<a href="https://github.com/like413/MACN">Code</a>]
    </td>
</tr>
</table>
<br />

</ul>
</ul>
<p></p>

<A NAME="#Co-authors"><h2>ğŸ¤ Co-authors</h2></A>
<font size="3"> 
<ul>
<li><strong>Professors:</strong> 
    <a href="https://web.xidian.edu.cn/wangdi/" target="_blank">Di Wang</a> (ç‹ç¬›), 
    <a href="https://see.xidian.edu.cn/faculty/xbgao/" target="_blank">Xinbo Gao</a> (é«˜æ–°æ³¢), 
    <a href="https://www.chuatatseng.com/" target="_blank">Tat-Seng Chua </a> (è”¡è¾¾æˆ), 
    <a href="https://web.xidian.edu.cn/liugang/" target="_blank">Gang Liu</a> (åˆ˜åˆš), 
    ...
</li>
<li><strong>Ph.D. students:</strong> 
    <a href="https://scholar.google.com/citations?user=gtMf_HIAAAAJ&hl=zh-CN" target="_blank">Yiming Zhang</a> (å¼ ç›Šæ˜), 
    <a href="https://scholar.google.com/citations?user=JWwU2TkAAAAJ&hl=zh-CN" target="_blank">Min Dang</a> (å…šæ•), 
    ...
</li>
</ul>
</font>
<br />

<A NAME="Membership"><h2>ğŸ« Membership</h2></A>
<font size="3"> 
<ul>
<li>China Computer Federation (CCF), Student Member</li>
<li>Computer Vision Foundation (CVF), Student Member</li>
<li>IEEE Geoscience and Remote Sensing Society (GRSS), Student Member</li>
<li>IEEE, Graduate Student Member</li>
</ul>
</font>
<br />

<A NAME="Academic Service"><h2>ğŸ’¼ Academic Service</h2></A>
<font size="3"> 
<ul>
<li><b>Journal Reviewer:</b> IEEE TPAMI, IEEE TIP, IEEE TNNLS, IEEE TCSVT, IEEE TGRS, IEEE JSTARS, IEEE GRSL, Pattern Recognition, Information Fusion, etc.</li>
<li><b>Conference Reviewer:</b> CVPR, ICCV, NeurIPS, ICLR, ACM MM, AAAI, IGARSS, etc.</li>
</ul>
</font>
<br />
 
<A NAME="Awards"><h2>ğŸ† Awards</h2></A>
<font size="3"> 
<div class="news" style="overflow:auto; height:200px; Width:99%;">
<ul>
<li>2025.12, &nbsp;Huawei Scholarship, Xidian University | <font style="font-family:Microsoft YaHei">è¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦ åä¸ºå¥–å­¦é‡‘ï¼ˆ<strong>å…¨æ ¡ä»…3äºº</strong>ï¼‰</font></li>
<li>2025.11, &nbsp;CCF ICCC 2025 <strong>A-Class Paper Award</strong> | <font style="font-family:Microsoft YaHei">ä¸­å›½è®¡ç®—æœºå­¦ä¼š ä¸­å›½å·¥ä¸šè®¡ç®—æœºå¤§ä¼š <strong>Aç±»è®ºæ–‡å¥–</strong></font></li>
<li>2025.04, &nbsp;<strong>First Prize</strong> Academic Scholarship, Xidian University | <font style="font-family:Microsoft YaHei">è¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦ å­¦ä¸šå¥–å­¦é‡‘<strong>ä¸€ç­‰å¥–</strong></font></li>
<li>2025.01, &nbsp;<strong>Grand Prize</strong> Tencent Scholarship, Xidian University | <font style="font-family:Microsoft YaHei">è¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦ è…¾è®¯å¥–å­¦é‡‘<strong>ç‰¹ç­‰å¥–</strong>ï¼ˆ<strong>å…¨æ ¡ä»…5äºº</strong>ï¼‰</font></li>
<li>2024.05, &nbsp;<strong>First Prize</strong> Academic Scholarship, Xidian University | <font style="font-family:Microsoft YaHei">è¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦ å­¦ä¸šå¥–å­¦é‡‘<strong>ä¸€ç­‰å¥–</strong></font></li>
<li>2023.12, &nbsp;<strong>First Prize</strong> Tencent Scholarship, Xidian University | <font style="font-family:Microsoft YaHei">è¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦ è…¾è®¯å¥–å­¦é‡‘<strong>ä¸€ç­‰å¥–</strong></font></li>
<li>2023.05, &nbsp;<strong>Second Prize</strong> Academic Scholarship, Xidian University | <font style="font-family:Microsoft YaHei">è¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦ å­¦ä¸šå¥–å­¦é‡‘äºŒç­‰å¥–</font></li>
<li>...</li>
</ul>
</div>
</font>
<br />
 
<A NAME="Conference Experience"><h2>ğŸ“† Conference Experience</h2></A>
<font size="3"> 
<div class="news" style="overflow:auto; height:200px; Width:99%;">
<ul>
<li>2025å¹´10æœˆ31-11æœˆ2æ—¥, <a href= "https://ccf.org.cn/ICCC2025" target="_blank"><u>èµ´æ˜†æ˜å‚åŠ ä¸­å›½è®¡ç®—æœºå­¦ä¼šä¸­å›½å·¥ä¸šè®¡ç®—æœºå¤§ä¼š, ICCC 2025</u></a>  </li>
<li>2025å¹´9æœˆ18-21æ—¥, <a href= "http://youth.csig.org.cn/CSIG2025/" target="_blank"><u>èµ´é’å²›å‚åŠ ä¸­å›½å›¾è±¡å›¾å½¢å­¦å­¦ä¼šé’å¹´ç§‘å­¦å®¶ä¼šè®®, 2025</u></a>  </li>
<li>2025å¹´7æœˆ18-20æ—¥, <a href= "https://www.airs.top/" target="_blank"><u>èµ´æ˜†æ˜å‚åŠ äººå·¥æ™ºèƒ½ä¸é¥æ„Ÿç§‘å­¦äº¤å‰è®ºå›, AIRS 2025</u></a>  </li>
<li>2025å¹´6æœˆ13-15æ—¥, <a href= "https://yb.cie.org.cn/" target="_blank"><u>èµ´æ­å·å‚åŠ ä¸­å›½ç”µå­å­¦ä¼šä¼˜åšè®ºå›, 2025</u></a>  </li>
<li>2025å¹´6æœˆ6-8æ—¥, <a href= "https://valser.org/2025/#/" target="_blank"><u>èµ´ç æµ·å‚åŠ ç¬¬åäº”å±Šè§†è§‰ä¸å­¦ä¹ é’å¹´å­¦è€…ç ”è®¨ä¼š, VALSE 2025</u></a>  </li>
<li>2024å¹´6æœˆ16-22æ—¥, <a href= "https://cvpr.thecvf.com/" target="_blank"><u>èµ´ç¾å›½è¥¿é›…å›¾å‚åŠ ç¬¬å››åä¸€å±Šå›½é™…è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®, CVPR 2024</u></a>  </li>
<li>2024å¹´5æœˆ5-7æ—¥, <a href= "https://valser.org/2024/#/" target="_blank"><u>èµ´é‡åº†å‚åŠ ç¬¬åå››å±Šè§†è§‰ä¸å­¦ä¹ é’å¹´å­¦è€…ç ”è®¨ä¼š, VALSE 2024</u></a>  </li>
<li>2023å¹´12æœˆ28-31æ—¥, <a href= "http://youth.csig.org.cn/CSIG2023/" target="_blank"><u>èµ´å¹¿å·å‚åŠ ç¬¬åä¹å±Šä¸­å›½å›¾åƒå›¾å½¢å­¦å­¦ä¼šé’å¹´ç§‘å­¦å®¶ä¼šè®®, 2023</u></a>  </li>
<li>2023å¹´10æœˆ13-15æ—¥, <a href= "http://prcv2023.xmu.edu.cn" target="_blank"><u>èµ´å¦é—¨å‚åŠ ç¬¬å…­å±Šä¸­å›½æ¨¡å¼è¯†åˆ«ä¸è®¡ç®—æœºè§†è§‰å¤§ä¼š, PRCV 2023</u></a>  </li>  
</ul>
</div>
</font>
<br>  


<script>
$(function(){
    $(window).scroll(function(){  //If scroll
        var scrollt = document.documentElement.scrollTop + document.body.scrollTop; //Getting Height after scroll
        if( scrollt >400 )
        {  
            $("#back_top").fadeIn(400); 
        }
        else
        {
            $("#back_top").stop().fadeOut(400);
        }
    });

    $("#back_top").click(function(){ 

        $("html,body").animate({scrollTop:"0px"}, 200);

    }); 

});
</script>

 
</body>
</html>
